"""
HP URL ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å…¬å¼HPã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€æœ€é©ãªURLã‚’åˆ¤å®š
"""

import re
import pykakasi
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from urllib.parse import urlparse
from rapidfuzz import fuzz
from rapidfuzz import utils as fuzz_utils

from .logger_config import get_logger
from .search_agent import SearchResult, CompanyInfo
from .utils import StringUtils, URLUtils

logger = get_logger(__name__)

@dataclass
class HPCandidate:
    """HPå€™è£œã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    url: str
    title: str
    description: str
    search_rank: int
    query_pattern: str
    domain_similarity: float
    is_top_page: bool
    total_score: float
    judgment: str  # 'è‡ªå‹•æ¡ç”¨', 'è¦ç¢ºèª', 'æ‰‹å‹•ç¢ºèª'
    score_details: Dict[str, Any]

@dataclass 
class ScoringConfig:
    """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¨­å®š"""
    # é‡ã¿ä»˜ã‘è¨­å®š
    top_page_bonus: int = 5
    domain_exact_match: int = 5
    domain_similar_match: int = 3
    tld_co_jp: int = 3
    tld_com_net: int = 1
    official_keyword_bonus: int = 2
    search_rank_bonus: int = 3
    path_depth_penalty_factor: int = -10 # ç¾çŠ¶ç›´æ¥ã¯ä½¿ã‚ã‚Œã¦ã„ãªã„ãŒå°†æ¥ç”¨
    domain_jp_penalty: int = -2
    path_keyword_penalty: int = -2
    
    # åˆ¤å®šé–¾å€¤
    auto_adopt_threshold: int = 9
    needs_review_threshold: int = 6
    similarity_threshold_domain: int = 80

class HPScorer:
    """HP URLã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: ScoringConfig, blacklist_domains: Set[str] = None, penalty_paths: List[str] = None):
        self.config = config
        self.blacklist_domains = blacklist_domains if blacklist_domains is not None else set()
        self.penalty_paths = penalty_paths if penalty_paths is not None else []
        self.string_utils = StringUtils()
        self.url_utils = URLUtils()
        
        # pykakasi ã‚³ãƒ³ãƒãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆv2.0+ New APIï¼‰
        self._kks = pykakasi.kakasi()
    
    def _romanize(self, text: str) -> str:
        """
        æ—¥æœ¬èªã‚’ãƒ­ãƒ¼ãƒå­—ã¸å¤‰æ›ï¼ˆpykakasi v2.0+ New APIï¼‰
        
        Args:
            text: å¤‰æ›å¯¾è±¡ã®æ—¥æœ¬èªæ–‡å­—åˆ—
        
        Returns:
            ãƒ­ãƒ¼ãƒå­—å¤‰æ›ã•ã‚ŒãŸæ–‡å­—åˆ—
        """
        try:
            if not text:
                return ""
            
            # v2.0+ New API: convertãƒ¡ã‚½ãƒƒãƒ‰ã§è¾æ›¸ãƒªã‚¹ãƒˆã‚’å–å¾—
            result = self._kks.convert(text)
            romanized = ''.join([item['hepburn'] for item in result])
            
            # å°æ–‡å­—ã«çµ±ä¸€ã—ã€ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
            return romanized.lower().strip()
            
        except Exception as e:
            logger.warning(f"ãƒ­ãƒ¼ãƒå­—å¤‰æ›ã‚¨ãƒ©ãƒ¼: text='{text}' - {e}")
            return ""
    
    def _enhanced_clean_company_name(self, company_name: str) -> str:
        """
        ä¼æ¥­åã®å¼·åŒ–ã•ã‚ŒãŸæ­£è¦åŒ–å‡¦ç†
        æ³•äººæ¥å°¾èªã‚’é™¤å»ã—ã¤ã¤ã€æ—¥æœ¬èªæœ¬ä½“ã¯ä¿æŒ
        
        Args:
            company_name: åŸä¼æ¥­å
        
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸä¼æ¥­å
        """
        if not company_name:
            return ""
        
        # åŸºæœ¬ã®æ­£è¦åŒ–ï¼ˆã€ã€‘é™¤å»ã€ç©ºç™½æ­£è¦åŒ–ï¼‰
        cleaned = self.string_utils.clean_company_name(company_name)
        
        # æ³•äººæ¥å°¾èªã‚’é™¤å»
        cleaned = self.string_utils.remove_legal_suffixes(cleaned)
        
        # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
        import unicodedata
        cleaned = unicodedata.normalize('NFKC', cleaned)
        
        # è¨˜å·ã‚’é™¤å»ï¼ˆãŸã ã—ã€æ—¥æœ¬èªæ–‡å­—ã¯ä¿æŒï¼‰
        # è‹±æ•°å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€ç©ºç™½ã®ã¿æ®‹ã™
        cleaned = re.sub(r'[^\w\sã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾¯]', '', cleaned)
        
        # ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned
    
    def _calculate_domain_similarity(self, company_name: str, url: str) -> float:
        """
        ãƒ‰ãƒ¡ã‚¤ãƒ³åã¨ä¼æ¥­åã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆèªå¹¹ã‚¹ãƒ—ãƒªãƒƒãƒˆå¼·åŒ–ç‰ˆï¼‰
        æ—¥æœ¬èªâ†’ãƒ­ãƒ¼ãƒå­—å¤‰æ›ã¨è¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨
        """
        try:
            # ä¼æ¥­åã®æ­£è¦åŒ–
            cleaned_name = self._enhanced_clean_company_name(company_name)
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³åã®å–å¾—
            domain = self.url_utils.get_domain(url)
            domain_without_tld = domain.split('.')[0]
            
            # æ¯”è¼ƒå€™è£œã‚’æº–å‚™
            candidates = []
            
            # 1. åŸä¼æ¥­åï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰
            if cleaned_name:
                candidates.append(cleaned_name)
            
            # 2. ãƒ­ãƒ¼ãƒå­—å¤‰æ›ç‰ˆï¼ˆæ—¥æœ¬èªãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if re.search(r'[ã‚-ã‚“ã‚¢-ãƒ¶ãƒ¼ä¸€-é¾¯]', cleaned_name):
                romanized_name = self._romanize(cleaned_name)
                if romanized_name and romanized_name != cleaned_name.lower():
                    candidates.append(romanized_name)
            
            # 3. ã‚«ã‚¿ã‚«ãƒŠéƒ¨åˆ†ã®ã¿æŠ½å‡ºã—ã¦ãƒ­ãƒ¼ãƒå­—å¤‰æ›
            katakana_only = self.string_utils.extract_katakana(company_name)
            if katakana_only:
                romanized_katakana = self._romanize(katakana_only)
                if romanized_katakana and romanized_katakana not in candidates:
                    candidates.append(romanized_katakana)
                    
            # 4. è‹±èªã®å ´åˆã¯å°æ–‡å­—åŒ–ã—ãŸç‰ˆã‚‚è¿½åŠ 
            if re.search(r'[a-zA-Z]', cleaned_name):
                lower_name = cleaned_name.lower()
                if lower_name not in candidates:
                    candidates.append(lower_name)
            
            # ğŸš€ 5. èªå¹¹ã‚¹ãƒ—ãƒªãƒƒãƒˆå¼·åŒ–ï¼ˆNEWï¼‰
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å˜èªã«åˆ†å‰²ã—ã¦ãã‚Œãã‚Œã¨æ¯”è¼ƒ
            domain_tokens = self._split_domain_tokens(domain_without_tld)
            
            # å€™è£œãŒç©ºã®å ´åˆã¯0ã‚’è¿”ã™
            if not candidates:
                logger.debug(f"[SIM] æ¯”è¼ƒå€™è£œãªã—: name='{company_name}' -> cleaned='{cleaned_name}'")
                return 0.0
            
            # å„å€™è£œã§ã‚¹ã‚³ã‚¢è¨ˆç®—
            best_score = 0.0
            best_candidate = ""
            scores_log = []
            
            for candidate in candidates:
                if not candidate:
                    continue
                
                # å¾“æ¥ã®å…¨ä½“æ¯”è¼ƒ
                wratio_score = fuzz.WRatio(
                    candidate, domain_without_tld,
                    processor=fuzz_utils.default_process
                )
                
                token_sort_score = fuzz.token_sort_ratio(
                    candidate, domain_without_tld,
                    processor=fuzz_utils.default_process
                )
                
                # ğŸš€ èªå¹¹ã‚¹ãƒ—ãƒªãƒƒãƒˆæ¯”è¼ƒï¼ˆNEWï¼‰
                split_score = self._calculate_token_split_similarity(candidate, domain_tokens)
                
                # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’æ¡ç”¨
                score = max(wratio_score, token_sort_score, split_score)
                scores_log.append(f"{candidate}â†’{score}(W:{wratio_score}/T:{token_sort_score}/S:{split_score})")
                
                if score > best_score:
                    best_score = score
                    best_candidate = candidate
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
            logger.debug(f"[SIM] name='{company_name}' domain='{domain_without_tld}' tokens={domain_tokens} "
                        f"best={best_score} via '{best_candidate}' scores=[{', '.join(scores_log)}]")
            
            return float(best_score)
            
        except Exception as e:
            logger.warning(f"ãƒ‰ãƒ¡ã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: Name='{company_name}', URL='{url}' - {e}", exc_info=True)
            return 0.0
    
    def _split_domain_tokens(self, domain: str) -> List[str]:
        """
        ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æ„å‘³ã®ã‚ã‚‹å˜èªãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²
        
        Args:
            domain: ãƒ‰ãƒ¡ã‚¤ãƒ³åï¼ˆTLDé™¤å»æ¸ˆã¿ï¼‰
        
        Returns:
            åˆ†å‰²ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        try:
            # åŒºåˆ‡ã‚Šæ–‡å­—ã§ã®åˆ†å‰²
            tokens = re.split(r'[-_\.]', domain.lower())
            
            # ç©ºæ–‡å­—åˆ—ã¨çŸ­ã™ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
            tokens = [token for token in tokens if token and len(token) >= 2]
            
            return tokens
            
        except Exception as e:
            logger.warning(f"ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã‚¨ãƒ©ãƒ¼: domain='{domain}' - {e}")
            return [domain.lower()]
    
    def _calculate_token_split_similarity(self, candidate: str, domain_tokens: List[str]) -> float:
        """
        èªå¹¹ã‚¹ãƒ—ãƒªãƒƒãƒˆé¡ä¼¼åº¦è¨ˆç®—
        
        Args:
            candidate: æ¯”è¼ƒå¯¾è±¡ã®ä¼æ¥­åå€™è£œ
            domain_tokens: ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ
        
        Returns:
            æœ€é«˜é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        """
        try:
            if not domain_tokens or not candidate:
                return 0.0
            
            # å€™è£œæ–‡å­—åˆ—ã‚‚åˆ†å‰²
            candidate_tokens = re.split(r'[\s\-_]', candidate.lower())
            candidate_tokens = [token for token in candidate_tokens if token and len(token) >= 2]
            
            if not candidate_tokens:
                candidate_tokens = [candidate.lower()]
            
            max_score = 0.0
            
            # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³ã¨å„å€™è£œãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€é«˜é¡ä¼¼åº¦ã‚’è¨ˆç®—
            for domain_token in domain_tokens:
                for candidate_token in candidate_tokens:
                    # å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
                    if domain_token == candidate_token:
                        max_score = max(max_score, 100.0)
                        continue
                    
                    # fuzzy ãƒãƒƒãƒãƒ³ã‚°
                    token_score = fuzz.ratio(candidate_token, domain_token)
                    max_score = max(max_score, token_score)
            
            # å…¨ä½“ã¨ã®æ¯”è¼ƒã‚‚å®Ÿæ–½
            full_domain = ''.join(domain_tokens)
            full_candidate = ''.join(candidate_tokens)
            full_score = fuzz.ratio(full_candidate, full_domain)
            max_score = max(max_score, full_score)
            
            return float(max_score)
            
        except Exception as e:
            logger.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒ—ãƒªãƒƒãƒˆé¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: candidate='{candidate}' tokens={domain_tokens} - {e}")
            return 0.0
    
    def calculate_score(self, search_result: SearchResult, company: CompanyInfo, query_pattern: str) -> Optional[HPCandidate]:
        """
        å˜ä¸€ã®æ¤œç´¢çµæœã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        
        Args:
            search_result: æ¤œç´¢çµæœ
            company: ä¼æ¥­æƒ…å ±
            query_pattern: ä½¿ç”¨ã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³
        
        Returns:
            HPCandidate ã¾ãŸã¯ Noneï¼ˆãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆç­‰ã§é™¤å¤–ã®å ´åˆï¼‰
        """
        try:
            if self._is_blacklisted_domain(search_result.url):
                logger.debug(f"ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³é™¤å¤–: {search_result.url}")
                return None
            
            # ğŸš€ æ­»æ´»ç¢ºèªï¼ˆNEWï¼‰
            if not self._is_reachable(search_result.url):
                logger.debug(f"æ­»æ´»ç¢ºèªå¤±æ•—ã€æ¸›ç‚¹å¯¾è±¡: {search_result.url}")
                # å®Œå…¨é™¤å¤–ã§ã¯ãªãå¤§å¹…æ¸›ç‚¹ã§å¯¾å¿œ
            
            score_details = {}
            total_score = 0
            
            is_top_page = self._is_top_page(search_result.url)
            if is_top_page:
                score_details["top_page"] = self.config.top_page_bonus
                total_score += self.config.top_page_bonus
            else:
                score_details["top_page"] = 0
            
            domain_similarity = self._calculate_domain_similarity(
                company.company_name, search_result.url
            )
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³å®Œå…¨ä¸€è‡´ã®åˆ¤å®šã‚’ã‚ˆã‚Šå³å¯†ã«ï¼ˆé¡ä¼¼åº¦95ä»¥ä¸Šãªã©ï¼‰
            if domain_similarity >= 95: 
                domain_score = self.config.domain_exact_match
            elif domain_similarity >= self.config.similarity_threshold_domain:
                domain_score = self.config.domain_similar_match
            else:
                domain_score = 0
            score_details["domain_similarity_score"] = domain_score
            total_score += domain_score
            
            tld_score = self._get_tld_score(search_result.url)
            score_details["tld_score"] = tld_score
            total_score += tld_score
            
            if self._has_official_keywords(search_result.title):
                official_score = self.config.official_keyword_bonus
                score_details["official_keyword"] = official_score
                total_score += official_score
            else:
                score_details["official_keyword"] = 0
            
            rank_bonus = self._get_search_rank_bonus(search_result.rank)
            score_details["search_rank"] = rank_bonus
            total_score += rank_bonus
            
            path_penalty = self._get_path_penalty(search_result.url)
            score_details["path_penalty"] = path_penalty
            total_score += path_penalty
            
            # ğŸ¯ åœ°åŸŸç‰¹å®šå¼·åŒ–ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            locality_score = self._calculate_locality_score(search_result, company)
            score_details["locality"] = locality_score
            total_score += locality_score
            
            # ğŸš« ãƒãƒ¼ã‚¿ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆå¼·åŒ–ç‰ˆï¼‰
            portal_penalty = self._get_enhanced_portal_penalty(search_result.url)
            score_details["portal_penalty"] = portal_penalty
            total_score += portal_penalty
            
            # ğŸš€ æ­»æ´»ç¢ºèªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆNEWï¼‰
            reachability_penalty = 0
            if not self._is_reachable(search_result.url):
                reachability_penalty = -6  # å¤§å¹…æ¸›ç‚¹
                logger.debug(f"æ­»æ´»ç¢ºèªå¤±æ•—ã«ã‚ˆã‚‹æ¸›ç‚¹: {search_result.url}")
            score_details["reachability_penalty"] = reachability_penalty
            total_score += reachability_penalty
            
            # ğŸš€ åœ°åŸŸãƒŸã‚¹ãƒãƒƒãƒå¼·åŒ–ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆNEWï¼‰
            # ä»–çœŒã§æ¤œå‡ºã•ã‚ŒãŸå ´åˆã€ã•ã‚‰ã«è¿½åŠ ãƒšãƒŠãƒ«ãƒ†ã‚£
            mismatch_penalty = self._calculate_geographic_mismatch_penalty(search_result, company)
            score_details["geographic_mismatch_penalty"] = mismatch_penalty
            total_score += mismatch_penalty
            
            # ğŸ”¥ æ±ç”¨èªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆNEWï¼‰
            # hairãƒ»groupãªã©æ±ç”¨èªã®ã¿ã®ä¸€è‡´ã¯æ¸›ç‚¹
            generic_penalty = self._calculate_generic_word_penalty(company.company_name, search_result.url)
            score_details["generic_word_penalty"] = generic_penalty
            total_score += generic_penalty
            
            # ğŸ”¥ HeadMatchãƒœãƒ¼ãƒŠã‚¹ï¼ˆNEWï¼‰- ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿
            # <title>ã‚¿ã‚°ã¨ã®ä¸€è‡´åˆ¤å®š
            head_match_bonus = self._calculate_head_match_bonus(company.company_name, search_result)
            score_details["head_match_bonus"] = head_match_bonus
            total_score += head_match_bonus
            
            judgment = self._determine_judgment(total_score)
            
            # è©³ç´°ãƒ­ã‚°å‡ºåŠ›ï¼ˆINFOãƒ¬ãƒ™ãƒ«ã«å¤‰æ›´ï¼‰
            logger.info(f"[SCORE] {company.company_name} -> {search_result.url[:50]}... "
                       f"total={total_score} judgment={judgment} "
                       f"top={score_details.get('top_page', 0)} domain={score_details.get('domain_similarity_score', 0)} "
                       f"head={score_details.get('head_match_bonus', 0)} portal={score_details.get('portal_penalty', 0)} "
                       f"rank={score_details.get('search_rank', 0)}")
            
            return HPCandidate(
                url=search_result.url,
                title=search_result.title,
                description=search_result.description,
                search_rank=search_result.rank,
                query_pattern=query_pattern,
                domain_similarity=domain_similarity, # å…ƒã®é¡ä¼¼åº¦(0-100)ã‚’æ ¼ç´
                is_top_page=is_top_page,
                total_score=total_score,
                judgment=judgment,
                score_details=score_details
            )
            
        except Exception as e:
            logger.error(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {search_result.url}, ä¼šç¤¾å: {company.company_name} - {e}", exc_info=True)
            return None
    
    def _calculate_locality_score(self, search_result: SearchResult, company: CompanyInfo) -> int:
        """
        åœ°åŸŸç‰¹å®šã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ3æ®µéšãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆï¼‰
        
        Args:
            search_result: æ¤œç´¢çµæœ
            company: ä¼æ¥­æƒ…å ±
        
        Returns:
            åœ°åŸŸã‚¹ã‚³ã‚¢
        """

        try:
            # ã‚¿ã‚¤ãƒˆãƒ« + èª¬æ˜æ–‡ã‚’çµåˆ
            text = f"{search_result.title} {search_result.description}".lower()
            score = 0
            
            # éƒ½é“åºœçœŒã‹ã‚‰å¸‚å¤–å±€ç•ªã‚’å–å¾—
            area_code = self._get_area_code_for_scoring(company.prefecture)
            
            # â‘  çœŒåãƒ»å¸‚åãŒãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ï¼ˆ+2ç‚¹ï¼‰
            prefecture_clean = company.prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', '')
            if prefecture_clean.lower() in text or company.prefecture.lower() in text:
                score += 2
            
            # â‘¡ å¸‚å¤–å±€ç•ªä¸€è‡´ï¼ˆ+3ç‚¹ï¼‰
            if area_code:
                import re
                # å¸‚å¤–å±€ç•ªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒã‚¤ãƒ•ãƒ³ã‚ã‚Šãªã—ä¸¡å¯¾å¿œï¼‰
                pattern = rf'{area_code}[-\s]?[0-9]{{7,8}}'
                if re.search(pattern, text):
                    score += 3
            
            # â‘¢ åœ°åŸŸä»¥å¤–ã®éƒ½é“åºœçœŒãƒŸã‚¹ãƒãƒƒãƒãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ-10ç‚¹ï¼‰
            other_prefecture_penalty = self._check_other_prefecture_penalty(text, company.prefecture)
            score += other_prefecture_penalty
            
            # â‘£ Webãƒšãƒ¼ã‚¸è§£æã«ã‚ˆã‚‹åœ°åŸŸåˆ¤å®šï¼ˆæƒ…å ±ãŒè–„ã„å ´åˆã®è£œå¼·ï¼‰
            if score == 0:  # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ã‹ã‚‰åœ°åŸŸæƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆ
                # ğŸš€ ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆã§ã¯åœ°åŸŸè§£æã‚’ç„¡åŠ¹åŒ–ï¼ˆNEWï¼‰
                domain = self.url_utils.get_domain(search_result.url).lower()
                is_portal = any(portal in domain for portal in [
                    'hotpepper.jp', 'rakuten.co.jp', 'minimodel.jp', 'relax.jp', 
                    'yahoo.co.jp', 'google.com', 'tabelog.com'
                ])
                
                if not is_portal:
                    web_location_score = self._calculate_web_location_score(search_result.url, company.prefecture)
                    score += web_location_score
                else:
                    logger.debug(f"ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆã®ãŸã‚åœ°åŸŸè§£æã‚’ã‚¹ã‚­ãƒƒãƒ—: {search_result.url}")
            
            return score
            
        except Exception as e:
            logger.warning(f"åœ°åŸŸã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {search_result.url} - {e}")
            return 0
    
    def _calculate_web_location_score(self, url: str, target_prefecture: str) -> int:
        """
        Webãƒšãƒ¼ã‚¸è§£æã«ã‚ˆã‚‹åœ°åŸŸã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ3æ®µéšãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        
        Args:
            url: è§£æå¯¾è±¡URL
            target_prefecture: ç›®æ¨™éƒ½é“åºœçœŒ
            
        Returns:
            åœ°åŸŸã‚¹ã‚³ã‚¢
        """
        try:
            from .web_content_analyzer import WebContentAnalyzer
            
            analyzer = WebContentAnalyzer(timeout=5)  # çŸ­ã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            location_info = analyzer.extract_location_info(url)
            
            if not location_info.prefecture:
                return 0
            
            # åœ°åŸŸä¸€è‡´åˆ¤å®š
            if location_info.prefecture == target_prefecture:
                # ç²¾åº¦ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸãƒœãƒ¼ãƒŠã‚¹
                if location_info.confidence_level == "high":
                    return 4  # JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆ+4ç‚¹ï¼‰
                elif location_info.confidence_level == "medium":
                    return 3  # HTMLãƒ•ãƒƒã‚¿ãƒ¼/æ­£è¦è¡¨ç¾ï¼ˆ+3ç‚¹ï¼‰
                elif location_info.confidence_level == "low":
                    return 2  # ãŠå•ã„åˆã‚ã›ãƒšãƒ¼ã‚¸ï¼ˆ+2ç‚¹ï¼‰
            else:
                # åœ°åŸŸä¸ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£
                if location_info.confidence_level == "high":
                    return -4  # JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆ-4ç‚¹ï¼‰
                elif location_info.confidence_level == "medium":
                    return -3  # HTMLãƒ•ãƒƒã‚¿ãƒ¼/æ­£è¦è¡¨ç¾ï¼ˆ-3ç‚¹ï¼‰
                elif location_info.confidence_level == "low":
                    return -2  # ãŠå•ã„åˆã‚ã›ãƒšãƒ¼ã‚¸ï¼ˆ-2ç‚¹ï¼‰
            
            return 0
            
        except Exception as e:
            logger.warning(f"Webåœ°åŸŸè§£æã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return 0
    
    def _get_area_code_for_scoring(self, prefecture: str) -> str:
        """
        éƒ½é“åºœçœŒã‹ã‚‰ä»£è¡¨çš„ãªå¸‚å¤–å±€ç•ªã‚’å–å¾—ï¼ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç”¨ï¼‰
        
        Args:
            prefecture: éƒ½é“åºœçœŒå
        
        Returns:
            å¸‚å¤–å±€ç•ªï¼ˆè©²å½“ãªã—ã®å ´åˆã¯ç©ºæ–‡å­—ï¼‰
        """
        area_code_map = {
            'æ„›çŸ¥çœŒ': '052',      # åå¤å±‹å¸‚åœ
            'æ±äº¬éƒ½': '03',       # æ±äº¬23åŒº
            'å¤§é˜ªåºœ': '06',       # å¤§é˜ªå¸‚
            'ç¥å¥ˆå·çœŒ': '045',    # æ¨ªæµœå¸‚
            'å…µåº«çœŒ': '078',      # ç¥æˆ¸å¸‚
            'äº¬éƒ½åºœ': '075',      # äº¬éƒ½å¸‚
            'ç¦å²¡çœŒ': '092',      # ç¦å²¡å¸‚
            'åŒ—æµ·é“': '011',      # æœ­å¹Œå¸‚
            'å®®åŸçœŒ': '022',      # ä»™å°å¸‚
            'åºƒå³¶çœŒ': '082',      # åºƒå³¶å¸‚
            'é™å²¡çœŒ': '054',      # é™å²¡å¸‚
            'åƒè‘‰çœŒ': '043',      # åƒè‘‰å¸‚
            'åŸ¼ç‰çœŒ': '048',      # ã•ã„ãŸã¾å¸‚
        }
        return area_code_map.get(prefecture, '')
    
    def _check_other_prefecture_penalty(self, text: str, target_prefecture: str) -> int:
        """
        ä»–ã®éƒ½é“åºœçœŒãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—
        
        Args:
            text: æ¤œç´¢çµæœã®ã‚¿ã‚¤ãƒˆãƒ«+èª¬æ˜æ–‡
            target_prefecture: ç›®æ¨™ã®éƒ½é“åºœçœŒ
        
        Returns:
            ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆ0 ã¾ãŸã¯ -10ï¼‰
        """
        try:
            # å…¨47éƒ½é“åºœçœŒãƒªã‚¹ãƒˆ
            all_prefectures = [
                'åŒ—æµ·é“', 'é’æ£®çœŒ', 'å²©æ‰‹çœŒ', 'å®®åŸçœŒ', 'ç§‹ç”°çœŒ', 'å±±å½¢çœŒ', 'ç¦å³¶çœŒ',
                'èŒ¨åŸçœŒ', 'æ ƒæœ¨çœŒ', 'ç¾¤é¦¬çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ',
                'æ–°æ½ŸçœŒ', 'å¯Œå±±çœŒ', 'çŸ³å·çœŒ', 'ç¦äº•çœŒ', 'å±±æ¢¨çœŒ', 'é•·é‡çœŒ', 'å²é˜œçœŒ',
                'é™å²¡çœŒ', 'æ„›çŸ¥çœŒ', 'ä¸‰é‡çœŒ', 'æ»‹è³€çœŒ', 'äº¬éƒ½åºœ', 'å¤§é˜ªåºœ', 'å…µåº«çœŒ',
                'å¥ˆè‰¯çœŒ', 'å’Œæ­Œå±±çœŒ', 'é³¥å–çœŒ', 'å³¶æ ¹çœŒ', 'å²¡å±±çœŒ', 'åºƒå³¶çœŒ', 'å±±å£çœŒ',
                'å¾³å³¶çœŒ', 'é¦™å·çœŒ', 'æ„›åª›çœŒ', 'é«˜çŸ¥çœŒ', 'ç¦å²¡çœŒ', 'ä½è³€çœŒ', 'é•·å´çœŒ',
                'ç†Šæœ¬çœŒ', 'å¤§åˆ†çœŒ', 'å®®å´çœŒ', 'é¹¿å…å³¶çœŒ', 'æ²–ç¸„çœŒ'
            ]
            
            # ç›®æ¨™éƒ½é“åºœçœŒä»¥å¤–ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for prefecture in all_prefectures:
                if prefecture != target_prefecture:
                    # éƒ½é“åºœçœŒåï¼ˆçœŒãªã—ï¼‰ã§ã‚‚ãƒã‚§ãƒƒã‚¯
                    prefecture_short = prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', '')
                    if prefecture.lower() in text.lower() or prefecture_short.lower() in text.lower():
                        return -100
            
            return 0
            
        except Exception as e:
            logger.warning(f"ä»–çœŒãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {target_prefecture} - {e}")
            return 0
    
    def _get_portal_domain_penalty(self, url: str) -> int:
        """
        ãƒãƒ¼ã‚¿ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¨ˆç®—
        
        Args:
            url: æ¤œç´¢çµæœã®URL
        
        Returns:
            ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆ0 ã¾ãŸã¯ -2ã€œ-4ï¼‰
        """
        try:
            domain = self.url_utils.get_domain(url).lower()
            
            # ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£ãƒãƒƒãƒ—
            portal_penalties = {
                'hotpepper.jp': -4,
                'beauty.hotpepper.jp': -4,
                'relax.jp': -3,
                'rakuten.co.jp': -2,
                'yahoo.co.jp': -2,
                'google.com': -2,
            }
            
            for portal_domain, penalty in portal_penalties.items():
                if portal_domain in domain:
                    return penalty
            
            return 0
            
        except Exception as e:
            logger.warning(f"ãƒãƒ¼ã‚¿ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return 0
    
    def _get_enhanced_portal_penalty(self, url: str) -> int:
        """
        ğŸ”¥ å®Œå…¨é™¤å¤–ç´šãƒãƒ¼ã‚¿ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        Args:
            url: æ¤œç´¢çµæœã®URL
        
        Returns:
            ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆ0 ã¾ãŸã¯ -100ï¼‰
        """
        try:
            domain = self.url_utils.get_domain(url).lower()
            
            # ğŸ”¥ ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆå®Œå…¨é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆ-100ç‚¹ï¼‰
            portal_domains = {
                # ç¾å®¹ç³»ãƒãƒ¼ã‚¿ãƒ«
                'beauty.hotpepper.jp',
                'hotpepper.jp', 
                'beauty.rakuten.co.jp',
                'rakuten.co.jp',
                'minimodel.jp',
                'relax.jp',
                'beauty.biglobe.ne.jp',
                'epark.jp',
                'salonia.com',
                
                # æ±ç”¨ãƒãƒ¼ã‚¿ãƒ«
                'yahoo.co.jp',
                'google.com',
                'gnaviapp.com', 
                'tabelog.com',
                'yelp.com',
                'itp.ne.jp',        # ã‚¿ã‚¦ãƒ³ãƒšãƒ¼ã‚¸
                'mapion.co.jp',     # ãƒãƒ”ã‚ªãƒ³
                'navitime.co.jp',   # ãƒŠãƒ“ã‚¿ã‚¤ãƒ 
                
                # SNSãƒ»ã¾ã¨ã‚ç³»
                'facebook.com',
                'instagram.com', 
                'twitter.com',
                'ameblo.jp',
                'fc2.com',
                'livedoor.jp',
                'blogger.com',
                'wordpress.com',
                
                # æ±‚äººç³»
                'rikunabi.com',
                'mynavi.jp',
                'indeed.com',
                'doda.jp',
                'baitoru.com',
                
                # ECãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ç³»
                'amazon.co.jp',
                'mercari.com',
                'kakaku.com',
                '@cosme.net',
            }
            
            for portal_domain in portal_domains:
                if portal_domain in domain:
                    logger.debug(f"ğŸ”¥ ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆå®Œå…¨é™¤å¤–: {domain} (-100ç‚¹)")
                    return -100
            
            return 0
            
        except Exception as e:
            logger.warning(f"ãƒãƒ¼ã‚¿ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return 0
    
    def _is_reachable(self, url: str, timeout: int = 4) -> bool:
        """
        URLã®æ­»æ´»ç¢ºèª
        
        Args:
            url: ç¢ºèªå¯¾è±¡ã®URL
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
        
        Returns:
            åˆ°é”å¯èƒ½ã‹ã©ã†ã‹
        """
        try:
            import requests
            response = requests.head(url, timeout=timeout, allow_redirects=True)
            return response.status_code < 400
        except Exception as e:
            logger.debug(f"æ­»æ´»ç¢ºèªå¤±æ•—: {url} - {e}")
            return False
    
    def _calculate_geographic_mismatch_penalty(self, search_result: SearchResult, company: CompanyInfo) -> int:
        """
        åœ°åŸŸãƒŸã‚¹ãƒãƒƒãƒå¼·åŒ–ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¨ˆç®—
        Webãƒšãƒ¼ã‚¸è§£æã§ä»–çœŒãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®è¿½åŠ ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        Args:
            search_result: æ¤œç´¢çµæœ
            company: ä¼æ¥­æƒ…å ±
        
        Returns:
            ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆ0 ã¾ãŸã¯ -5ã€œ-8ï¼‰
        """
        try:
            # Webãƒšãƒ¼ã‚¸è§£æã§åœ°åŸŸã‚’å–å¾—
            from .web_content_analyzer import WebContentAnalyzer
            
            analyzer = WebContentAnalyzer(timeout=3)
            location_info = analyzer.extract_location_info(search_result.url)
            
            if not location_info.prefecture:
                return 0
            
            # ç›®æ¨™çœŒã¨ä¸€è‡´ã—ãªã„å ´åˆ
            if location_info.prefecture != company.prefecture:
                # ğŸ”¥ åœ°åŸŸé•ã„ã¯å®Œå…¨é™¤å¤–ç´šã®ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼
                if location_info.confidence_level == "high":
                    penalty = -100  # JSON-LDç­‰ã§ç¢ºå®Ÿã«æ¤œå‡º â†’ å®Œå…¨é™¤å¤–
                elif location_info.confidence_level == "medium":
                    penalty = -50   # HTMLè§£æã§æ¤œå‡º â†’ å¤§å¹…æ¸›ç‚¹
                else:
                    penalty = -20   # é€£çµ¡å…ˆãƒšãƒ¼ã‚¸ã§æ¤œå‡º â†’ å³é‡æ¸›ç‚¹
                
                logger.debug(f"åœ°åŸŸãƒŸã‚¹ãƒãƒƒãƒãƒšãƒŠãƒ«ãƒ†ã‚£: {search_result.url} "
                           f"æ¤œå‡º={location_info.prefecture} vs ç›®æ¨™={company.prefecture} "
                           f"ä¿¡é ¼åº¦={location_info.confidence_level} ãƒšãƒŠãƒ«ãƒ†ã‚£={penalty}")
                
                return penalty
            
            return 0
            
        except Exception as e:
            logger.debug(f"åœ°åŸŸãƒŸã‚¹ãƒãƒƒãƒãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {search_result.url} - {e}")
            return 0
    
    def _calculate_generic_word_penalty(self, company_name: str, url: str) -> int:
        """
        æ±ç”¨èªãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—
        hairãƒ»groupãªã©æ±ç”¨èªã®ã¿ã®ä¸€è‡´ã¯æ¸›ç‚¹
        
        Args:
            company_name: ä¼æ¥­å
            url: æ¤œç´¢çµæœã®URL
        
        Returns:
            ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆ0 ã¾ãŸã¯ -5ï¼‰
        """
        try:
            # æ±ç”¨èªãƒªã‚¹ãƒˆ
            generic_words = {
                # ç¾å®¹ç³»æ±ç”¨èª
                'hair', 'salon', 'beauty', 'cut', 'style', 'nail', 'spa',
                'esthetic', 'relax', 'care', 'clinic', 'total', 'private',
                
                # ä¸€èˆ¬æ±ç”¨èª
                'group', 'company', 'corp', 'co', 'inc', 'ltd', 'shop',
                'store', 'center', 'studio', 'design', 'creative', 'pro',
                'plus', 'premium', 'select', 'special', 'new', 'fresh',
                'modern', 'urban', 'royal', 'grand', 'first', 'main',
                
                # åœ°åŸŸç³»æ±ç”¨èª
                'tokyo', 'osaka', 'nagoya', 'yokohama', 'kyoto', 'kobe',
                'shibuya', 'shinjuku', 'ikebukuro', 'ginza', 'omotesando'
            }
            
            # ä¼æ¥­åã‹ã‚‰è‹±èªéƒ¨åˆ†ã‚’æŠ½å‡º
            import re
            company_english = re.findall(r'[a-zA-Z]+', company_name.lower())
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‹ã‚‰è‹±èªéƒ¨åˆ†ã‚’æŠ½å‡º
            domain = self.url_utils.get_domain(url)
            domain_without_tld = domain.split('.')[0].lower()
            domain_words = re.findall(r'[a-zA-Z]+', domain_without_tld)
            
            if not company_english or not domain_words:
                return 0
            
            # ä¸€è‡´ã™ã‚‹å˜èªã‚’ãƒã‚§ãƒƒã‚¯
            matched_words = []
            for comp_word in company_english:
                for domain_word in domain_words:
                    if comp_word == domain_word and comp_word in generic_words:
                        matched_words.append(comp_word)
            
            # æ±ç”¨èªã®ã¿ã®ä¸€è‡´ã‹ãƒã‚§ãƒƒã‚¯
            if matched_words:
                # éæ±ç”¨èªã®ä¸€è‡´ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                non_generic_match = False
                for comp_word in company_english:
                    for domain_word in domain_words:
                        if comp_word == domain_word and comp_word not in generic_words:
                            non_generic_match = True
                            break
                    if non_generic_match:
                        break
                
                # æ±ç”¨èªã®ã¿ã®ä¸€è‡´ã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
                if not non_generic_match:
                    logger.debug(f"ğŸ”¥ æ±ç”¨èªã®ã¿ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£: {matched_words} (-5ç‚¹)")
                    return -5
            
            return 0
            
        except Exception as e:
            logger.warning(f"æ±ç”¨èªãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: company='{company_name}' url='{url}' - {e}")
            return 0
    
    def _calculate_head_match_bonus(self, company_name: str, search_result: SearchResult) -> int:
        """
        HeadMatchãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ç‰ˆï¼‰
        ä¼æ¥­åãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã§åˆ¤å®š
        
        Args:
            company_name: ä¼æ¥­å
            search_result: æ¤œç´¢çµæœ
        
        Returns:
            ãƒœãƒ¼ãƒŠã‚¹ãƒ»ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆ-5ï½+10ï¼‰
        """
        try:
            # æ¯”è¼ƒå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã®åé›†ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ï¼‰
            head_texts = []
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
            if search_result.title:
                head_texts.append(search_result.title)
                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—æ•°ã‚’ç¢ºèª
                logger.debug(f"HeadMatch - ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—æ•°: {len(search_result.title)} - '{search_result.title}'")
            
            if not head_texts:
                logger.debug("HeadMatch - ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                return 0
            
            # ä¼æ¥­åã®æ­£è¦åŒ–ï¼ˆHeadMatchå°‚ç”¨è¿½åŠ å‡¦ç†ï¼‰
            cleaned_company = self._enhanced_clean_company_name(company_name)
            
            # ğŸš€ æ¥­ç¨®æ¥é ­èªã‚’é™¤å»ï¼ˆHeadMatchå°‚ç”¨ï¼‰
            business_prefixes = ['ç¾å®¹å®¤', 'ã‚µãƒ­ãƒ³', 'ãƒ˜ã‚¢ã‚µãƒ­ãƒ³', 'ç†å®¹å®¤', 'ç†å®¹åº—', 'ãƒãƒ¼ãƒãƒ¼', 'ã‚¨ã‚¹ãƒ†', 'ãƒã‚¤ãƒ«']
            for prefix in business_prefixes:
                if cleaned_company.startswith(prefix):
                    cleaned_company = cleaned_company[len(prefix):].strip()
                    break
            
            # ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆåˆ¤å®š
            domain = self.url_utils.get_domain(search_result.url).lower()
            is_portal = any(portal in domain for portal in [
                'hotpepper.jp', 'rakuten.co.jp', 'minimodel.jp', 'relax.jp', 
                'yahoo.co.jp', 'google.com', 'tabelog.com', 'gnavi.co.jp',
                'beauty.hotpepper.jp', 'hairbook.jp'
            ])
            
            # ä¼æ¥­åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            found_in_text = False
            matched_text = ""
            
            for text in head_texts:
                if not text:
                    continue
                
                # HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–
                import re
                clean_text = re.sub(r'<[^>]+>', '', text)
                clean_text = re.sub(r'\s+', ' ', clean_text).strip()
                
                # ä¼æ¥­åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ï¼‰
                
                # 1. å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                if cleaned_company.lower() in clean_text.lower():
                    found_in_text = True
                    matched_text = clean_text[:50] + "..." if len(clean_text) > 50 else clean_text
                    break
                
                # 2. ã‚ˆã‚ŠæŸ”è»Ÿãªä¸€è‡´ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ»è¨˜å·ç„¡è¦–ï¼‰
                company_no_space = re.sub(r'[\s\-_Ã—&]', '', cleaned_company.lower())
                text_no_space = re.sub(r'[\s\-_Ã—&]', '', clean_text.lower())
                
                if company_no_space in text_no_space and len(company_no_space) >= 3:
                    found_in_text = True
                    matched_text = clean_text[:50] + "..." if len(clean_text) > 50 else clean_text
                    break
                
                # 3. ğŸš€ éƒ¨åˆ†å˜èªä¸€è‡´ï¼ˆNEWï¼‰- ä¼æ¥­åã®é‡è¦éƒ¨åˆ†ã ã‘ã§ã‚‚ä¸€è‡´
                # ä¼æ¥­åã‚’å˜èªã«åˆ†å‰²ã—ã¦ã€å„å˜èªãŒãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                company_words = [w.strip() for w in re.split(r'[\s\-_Ã—&ãƒ»]', cleaned_company) if w.strip() and len(w.strip()) >= 2]
                company_words_lower = [w.lower() for w in company_words]
                text_lower = clean_text.lower()
                
                matched_words = []
                for word in company_words_lower:
                    if word in text_lower and word not in ['åº—', 'ç¾å®¹å®¤', 'ã‚µãƒ­ãƒ³', 'hair', 'beauty']:  # æ±ç”¨èªã¯é™¤å¤–
                        matched_words.append(word)
                
                # é‡è¦å˜èªãŒ1ã¤ä»¥ä¸Šä¸€è‡´ã—ã€3æ–‡å­—ä»¥ä¸Šã®å ´åˆ
                if matched_words and any(len(w) >= 3 for w in matched_words):
                    found_in_text = True
                    matched_text = clean_text[:50] + "..." if len(clean_text) > 50 else clean_text
                    break
            
            # å¾—ç‚¹åˆ¤å®š
            if found_in_text:
                if is_portal:
                    # ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆã®å ´åˆã¯ä¸­ç¨‹åº¦ã®ãƒœãƒ¼ãƒŠã‚¹
                    logger.info(f"ğŸ”¥ HeadMatch(ãƒãƒ¼ã‚¿ãƒ«): '{matched_text}' (+5ç‚¹)")
                    return 5
                else:
                    # å…¬å¼ã‚µã‚¤ãƒˆã®å¯èƒ½æ€§ãŒé«˜ã„å ´åˆã¯é«˜å¾—ç‚¹
                    logger.info(f"ğŸ”¥ HeadMatch(å…¬å¼å¯èƒ½æ€§): '{matched_text}' (+10ç‚¹)")
                    return 10
            else:
                # ä¼æ¥­åãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆ
                if is_portal:
                    # ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆãªã‚‰è»½å¾®ãªãƒšãƒŠãƒ«ãƒ†ã‚£
                    logger.info(f"HeadMatch(ãƒãƒ¼ã‚¿ãƒ«ãƒ»ä¸ä¸€è‡´): '{head_texts[0][:30] if head_texts else 'N/A'}...' (-2ç‚¹)")
                    return -2
                else:
                    # å…¬å¼ã‚µã‚¤ãƒˆãªã®ã«ä¼æ¥­åãŒãªã„å ´åˆã¯é‡ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
                    logger.info(f"ğŸ”¥ HeadMatch(ä¸ä¸€è‡´): '{head_texts[0][:30] if head_texts else 'N/A'}...' (-5ç‚¹)")
                    return -5
            
        except Exception as e:
            logger.warning(f"HeadMatchãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: company='{company_name}' url='{search_result.url}' - {e}")
            return 0
    
    def score_multiple_candidates(self, search_results: Dict[str, List[SearchResult]], 
                                company: CompanyInfo) -> List[HPCandidate]:
        all_candidates = []
        for query_pattern, results in search_results.items():
            for result in results:
                candidate = self.calculate_score(result, company, query_pattern)
                if candidate:
                    all_candidates.append(candidate)
        all_candidates.sort(key=lambda x: x.total_score, reverse=True)
        return all_candidates
    
    def get_best_candidate(self, search_results: Dict[str, List[SearchResult]], 
                         company: CompanyInfo) -> Optional[HPCandidate]:
        candidates = self.score_multiple_candidates(search_results, company)
        if not candidates:
            return None
        return candidates[0]
    
    def _is_blacklisted_domain(self, url: str) -> bool:
        try:
            domain = self.url_utils.get_domain(url) # get_domainã¯æ—¢ã«wwwé™¤å»ã¨å°æ–‡å­—åŒ–ã‚’è¡Œã†
            return domain in self.blacklist_domains
        except Exception as e:
            logger.warning(f"ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®šã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return False # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã€ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã§ã¯ãªã„ã¨ã™ã‚‹
    
    def _is_top_page(self, url: str) -> bool:
        try:
            path_depth = self.url_utils.get_path_depth(url)
            return path_depth == 0
        except Exception as e:
            logger.warning(f"ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸åˆ¤å®šã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return False # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã§ã¯ãªã„ã¨ã™ã‚‹
    
    def _has_official_keywords(self, text: str) -> bool:
        if not text:
            return False
        text_lower = text.lower()
        official_keywords = ['å…¬å¼', 'official', 'ã‚ªãƒ•ã‚£ã‚·ãƒ£ãƒ«', 'æ­£å¼']
        return any(keyword in text_lower for keyword in official_keywords)
    
    def _get_tld_score(self, url: str) -> int:
        """
        ğŸ”¥ TLDã‚¹ã‚³ã‚¢æ”¹é©ç‰ˆ
        co.jpã®åŠ ç‚¹ã¯æ’¤å»ƒã€æ€ªã—ã„TLDã®ã¿æ¸›ç‚¹
        """
        try:
            domain = self.url_utils.get_domain(url) # æ—¢ã«å°æ–‡å­—åŒ–ã•ã‚Œã¦ã„ã‚‹
            
            # ğŸ”¥ æ€ªã—ã„TLDã®ã¿ãƒšãƒŠãƒ«ãƒ†ã‚£
            suspicious_tlds = {'.tk', '.ml', '.ga', '.cf', '.xyz', '.click', '.download'}
            
            for tld in suspicious_tlds:
                if domain.endswith(tld):
                    logger.debug(f"ğŸ”¥ æ€ªã—ã„TLDæ¸›ç‚¹: {domain} (-3ç‚¹)")
                    return -3
            
            # ãã®ä»–ã®TLDï¼ˆ.co.jp, .com, .net, .jpç­‰ï¼‰ã¯å…¨ã¦0ç‚¹
            return 0
            
        except Exception as e:
            logger.warning(f"TLDã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return 0
    
    def _get_search_rank_bonus(self, rank: int) -> int:
        if 1 <= rank <= 3:
            return self.config.search_rank_bonus
        return 0
    
    def _get_path_penalty(self, url: str) -> int:
        try:
            parsed_url = urlparse(url)
            path = parsed_url.path.lower()
            for penalty_keyword in self.penalty_paths:
                if penalty_keyword in path:
                    return self.config.path_keyword_penalty
            return 0
        except Exception as e:
            logger.warning(f"ãƒ‘ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return 0
    
    def _determine_judgment(self, total_score: float) -> str:
        if total_score >= self.config.auto_adopt_threshold:
            return "è‡ªå‹•æ¡ç”¨"
        elif total_score >= self.config.needs_review_threshold:
            return "è¦ç¢ºèª"
        elif total_score <= 0:
            return "è©²å½“ãªã—"  # ğŸš€ 0ç‚¹ä»¥ä¸‹ã¯è©²å½“ãªã—ï¼ˆNEWï¼‰
        else:
            return "æ‰‹å‹•ç¢ºèª"

def create_scorer_from_config(config: Dict[str, Any], blacklist_checker) -> HPScorer:
    """è¨­å®šã‹ã‚‰HPScorerã‚’ä½œæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°"""
    try:
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¨­å®šã®å–å¾—
        scoring_logic = config.get('scoring_logic', {})
        weights = scoring_logic.get('weights', {})
        
        scoring_config = ScoringConfig(
            top_page_bonus=weights.get('top_page', 5),
            domain_exact_match=weights.get('domain_exact_match', 5),
            domain_similar_match=weights.get('domain_similarity', 3),
            tld_co_jp=weights.get('tld_co_jp', 3),
            tld_com_net=weights.get('tld_com_net', 1),
            official_keyword_bonus=weights.get('official_keyword', 2),
            search_rank_bonus=weights.get('search_rank', 3),
            domain_jp_penalty=weights.get('domain_jp_penalty', -2),
            path_keyword_penalty=weights.get('path_penalty', -2),
            auto_adopt_threshold=scoring_logic.get('auto_adopt_threshold', 9),
            needs_review_threshold=scoring_logic.get('needs_review_threshold', 6),
            similarity_threshold_domain=scoring_logic.get('similarity_threshold_domain', 80)
        )
        
        # ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®å–å¾—
        blacklist_domains = blacklist_checker.get_blacklist_domains() if blacklist_checker else set()
        
        # ãƒšãƒŠãƒ«ãƒ†ã‚£ãƒ‘ã‚¹ã®å–å¾—
        penalty_paths = scoring_logic.get('penalty_paths', [
            'blog', 'news', 'recruit', 'contact', 'about'
        ])
        
        return HPScorer(
            config=scoring_config,
            blacklist_domains=blacklist_domains,
            penalty_paths=penalty_paths
        )
        
    except Exception as e:
        logger.error(f"HPScorerä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return HPScorer(
            config=ScoringConfig(),
            blacklist_domains=set(),
            penalty_paths=[]
        ) 